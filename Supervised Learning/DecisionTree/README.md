# Decision tree
* Non-parametric, supervised learning algorithms

* Given the training data, a decision tree algorithm divides the feature space into regions. For inference, we first see which region does the test data point fall in, and take the mean label values (regression) or the majority label value (classification).

* **Construction**: top-down, chooses a variable to split the data such that the target variables within each region are as homogeneous as possible. Two common metrics: gini impurity or information gain, won't matter much in practice.
Advantage: simply to understand & interpret, mirrors human decision making

### Click [here](https://github.com/syamkakarla98/Machine_Learning_Course/blob/master/Supervised%20Learning/DecisionTree/Decision_Tree.ipynb) to dig into the code.
